---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: observability
spec:
  interval: 5m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 32.2.1
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 10m
  test:
    enable: false # Enable helm test
  install:
    # Create new CRDs, update (replace) existing ones, but do not delete CRDs which no longer exist in the current helm release.
    crds: CreateReplace
    remediation: # perform remediation when helm install fails
      retries: 3
  upgrade:
    # Create new CRDs, update (replace) existing ones, but do not delete CRDs which no longer exist in the current helm release.
    crds: CreateReplace
    remediation: # perform remediation when helm upgrade fails
      retries: 3
      remediateLastFailure: true # remediate the last failure, when no retries remain
    cleanupOnFail: true
  rollback:
    timeout: 10m
    recreate: true
    cleanupOnFail: true
  dependsOn:
    - name: thanos
      namespace: observability
    - name: grafana
      namespace: observability
  timeout: 20m
  values:
    fullnameOverride: x
    # Disable kubeProxy whilst using Cilium as it's not deployed
    kubeProxy:
      enabled: false
    kubeScheduler:
      service:
        port: 10259
        targetPort: 10259
      serviceMonitor:
        insecureSkipVerify: true
    kubeEtcd:
      serviceMonitor:
        scheme: https
        serverName: localhost
        insecureSkipVerify: false
        caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
        certFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.crt
        keyFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.key
    alertmanager:
      fullnameOverride: alertmanager
    grafana:
      enabled: false
      forceDeployDashboards: true
      sidecar:
        dashboards:
          multicluster:
            etcd:
              enabled: true
    prometheus:
      fullnameOverride: prometheus
      ## Settings affecting prometheusSpec
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      prometheusSpec:
        retention: 2h
        ## Interval between consecutive scrapes.
        ## Defaults to 30s.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
        scrapeInterval: "60s"
        secrets:
          - etcd-client-cert
        externalUrl: https://prometheus.${CLUSTER_DOMAIN}
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 3096Mi
        thanos:
          image: raspbernetes/thanos:v0.23.1
          version: v0.17.1
          # Thanos chart will generate the secret: thanos
          objectStorageConfig:
            name: thanos
            key: object-store.yaml
          resources:
            requests:
              memory: 128Mi
              cpu: 100m
            limits:
              memory: 256Mi
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
    prometheus-node-exporter:
      fullnameOverride: node-exporter



apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: x-kubelet.rules
  namespace: observability
spec:
  groups:
  - name: kubelet.rules
    rules:
    - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m]))
        by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet",
        metrics_path="/metrics"})
      labels:
        quantile: "0.99"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m]))
        by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet",
        metrics_path="/metrics"})
      labels:
        quantile: "0.9"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m]))
        by (instance, le) * on(instance) group_left(node) kubelet_node_name{job="kubelet",
        metrics_path="/metrics"})
      labels:
        quantile: "0.5"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile


ts=2022-02-22T06:01:44.453Z caller=manager.go:609 level=warn component="rule manager"
group=kubelet.rules msg="Evaluating rule failed"
rule="record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile\nexpr: histogram_quantile(0.5, sum by(instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m]))\n  * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})\nlabels:\n  quantile: \"0.5\"\n"


err="found duplicate series for the match group {instance=\"192.168.1.161:10250\"} on the right hand-side of the operation: [{__name__=\"kubelet_node_name\", endpoint=\"https-metrics\", instance=\"192.168.1.161:10250\", job=\"kubelet\", metrics_path=\"/metrics\", namespace=\"kube-system\", node=\"k8s-controlplane-01\", service=\"x-kubelet\"}, {__name__=\"kubelet_node_name\", endpoint=\"https-metrics\", instance=\"192.168.1.161:10250\", job=\"kubelet\", metrics_path=\"/metrics\", namespace=\"kube-system\", node=\"k8s-controlplane-01\", service=\"prometheus-kubelet\"}];
many-to-many matching not allowed: matching labels must be unique on one side"
